{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees\n",
    "\n",
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "*With some material from [here](https://towardsdatascience.com/interactive-visualization-of-decision-trees-with-jupyter-widgets-ca15dd312084)*\n",
    "\n",
    "||continuous|categorical|\n",
    "|---|---|---|\n",
    "|**supervised**|**regression**|**classification**|\n",
    "|**unsupervised**|dimension reduction|clustering|\n",
    "\n",
    "Decision trees come in two flavors: classification and regression. We are only going to talk about classification decision trees. Regression decision trees are similar.\n",
    "\n",
    "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize a different criterion (discussed below)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification trees\n",
    "Note that classification trees easily handle **more than two response classes**! (How have other classification models we've seen handled this scenario?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import random\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import SVG\n",
    "# You may need to install the Python graphviz library. At the command line:\n",
    "#   pip install graphviz\n",
    "# You will also need to install the graphviz executables. You can use apt,\n",
    "# macports, or other installer for your system.\n",
    "from graphviz import Source\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an **example of a classification tree**, which predicts whether or not a patient who had chest pain has heart disease.\n",
    "\n",
    "Recall the Cleveland heart dataset:\n",
    "* age: age in years \n",
    "* sex: sex (1 = male; 0 = female) \n",
    "* cp: chest pain type \n",
    " * -- Value 1: typical angina \n",
    " * -- Value 2: atypical angina \n",
    " * -- Value 3: non-anginal pain \n",
    " * -- Value 4: asymptomatic \n",
    "* trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n",
    "* chol: serum cholestoral in mg/dl \n",
    "* fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "* restecg: resting electrocardiographic results \n",
    " * -- Value 0: normal \n",
    " * -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    " * -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "* thalach: maximum heart rate achieved \n",
    "* exang: exercise induced angina (1 = yes; 0 = no) \n",
    "* oldpeak = ST depression induced by exercise relative to rest \n",
    "* slope: the slope of the peak exercise ST segment \n",
    " * -- Value 1: upsloping \n",
    " * -- Value 2: flat \n",
    " * -- Value 3: downsloping \n",
    "* ca: number of major vessels (0-3) colored by flourosopy \n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "* num: diagnosis of heart disease (angiographic disease status) \n",
    " * -- Value 0: absence.\n",
    " * -- Value 1,2,3,4: presence of heart disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comma-separated values\n",
    "heart = pd.read_csv('cleveland.csv')\n",
    "\n",
    "# Rename 'num' column to 'disease'\n",
    "heart = heart.rename({'num':'disease'}, axis=1)\n",
    "heart['disease'] = heart.disease.apply(lambda x: min(x, 1))\n",
    "\n",
    "# create a list of feature columns\n",
    "feature_cols = ['age','sex','cp','trestbps','chol']\n",
    "\n",
    "# define X and y\n",
    "X = heart[feature_cols]\n",
    "y = heart.disease\n",
    "\n",
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)\n",
    "\n",
    "# # create a Graphviz file\n",
    "# with open(\"heart.dot\", 'w') as f:\n",
    "#     f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols, class_names=['0', '1'], filled = True)\n",
    "\n",
    "graph = Source(tree.export_graphviz(treeclf, out_file=None,\n",
    "                                    feature_names=feature_cols,\n",
    "                                    class_names=['0', '1'], filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "# Get a random sample to see how it does\n",
    "n = heart.shape[0]\n",
    "i = random.randint(0, n)\n",
    "# i = 254 # an interesting case\n",
    "# i = 15 # an interesting case\n",
    "sample = heart.iloc[i]\n",
    "print('Sample patient {} to test on. Tree classified as {}'.format(i, treeclf.predict([sample[feature_cols]])))\n",
    "display(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree plot, each node contains the condition (if/else rule) that splits the data, along with a series of other metrics of the node. Gini refers to the Gini impurity, a measure of the impurity of the node, i.e. how homogeneous are the samples within the node. We say that a node is pure when all its samples belong to the same class. In that case, there is no need for further split and this node is called a leaf. Samples is the number of instances in the node, while the value array shows the distribution of these instances per class. At the bottom we see the majority class of the node. When filled option of export_graphviz is set to True each node gets colored according to the majority class.\n",
    "\n",
    "While easy to understand, decision trees tend to over-fit the data by constructing complex models. Over fitted models will most likely not generalize well in “unseen” data. Two main approaches to prevent over-fitting are pre and post-pruning. Pre-pruning means restricting the depth of a tree prior to creation while post-pruning is removing non-informative nodes after the tree has been built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting criteria for classification trees\n",
    "\n",
    "Here are common options for the splitting criteria:\n",
    "\n",
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "- **Gini index:** measure of total variance across classes in a region\n",
    "- **cross-entropy:** numerically similar to Gini index, but uses logarithms\n",
    "\n",
    "Which to use?\n",
    "\n",
    "- When growing a tree, Gini index and cross-entropy are better measures of \"node purity\" than classification error rate. The Gini index is faster to compute than cross-entropy, so it is generally preferred (and is used by scikit-learn by default).\n",
    "- When pruning a tree, classification error rate is preferable in order to maximize predictive accuracy.\n",
    "\n",
    "Why do some splits result in leaves with the same predicted class?\n",
    "\n",
    "- The split was performed to increase node purity, even though it didn't reduce the classification error.\n",
    "- Node purity is important because we're interested in the class proportions among the observations in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical predictors\n",
    "\n",
    "Some implementations of classification trees will allow you to handle categorical predictors **without creating dummy variables**. When splitting on a categorical predictor, they will try splitting on **every possible combination of categories** to find the best split.\n",
    "\n",
    "**Unfortunately, scikit-learn's classification tree implementation does not support this approach.** Instead, here's how you can handle categorical predictors:\n",
    "\n",
    "- If a predictor only has **two possible values**, code it as a single binary variable (0 or 1). Since it's treated as a number, splits will naturally occur at 0.5.\n",
    "- If a predictor has **three or more possible values that are ordered**, code it as a single variable (1, 2, 3, etc). Splits will naturally occur at 1.5, 2.5, etc.\n",
    "- If a predictor has **three or more possible values that are unordered**, create dummy variables and drop one level as usual. The decision tree won't know that the dummy variables are related to one another, but that shouldn't matter in terms of predictive accuracy.\n",
    "- If a predictor has **thousands of possible unordered values**, then it may be best to code it as a single variable (1, 2, 3, etc) instead of using dummy variables to minimize the size of the resulting model. ([reference](http://stackoverflow.com/a/18736132/1636598))\n",
    "\n",
    "We'll see examples of these strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classification tree in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT4/master/data/titanic.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose our response and a few features, and decide whether we need to adjust them:\n",
    "\n",
    "- **survived:** This is our response, and is already encoded as 0=died and 1=survived.\n",
    "- **pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). They are ordered, so we'll leave them as-is.\n",
    "- **sex:** This is a binary category, so we should encode as 0=female and 1=male.\n",
    "- **age:** We need to fill in the missing values.\n",
    "- **embarked:** This is the port they emarked from. There are three unordered categories, so we'll create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sex feature\n",
    "titanic['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age\n",
    "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dummy variables using get_dummies\n",
    "pd.get_dummies(titanic.embarked, prefix='embarked').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "# note: axis=0 means rows, axis=1 means columns\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# define X and y\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a Graphviz file\n",
    "# with open(\"titanic.dot\", 'wb') as f:\n",
    "#     f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)\n",
    "\n",
    "graph = Source(tree.export_graphviz(treeclf, out_file=None,\n",
    "                                    feature_names=feature_cols,\n",
    "                                    class_names=['0', '1'], filled = True))\n",
    "display(SVG(graph.pipe(format='svg')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the split in the bottom right, which was made only to increase node purity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up decision trees\n",
    "\n",
    "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
    "- Non-parametric (will do better than linear regression if relationship between predictors and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"15_linear_vs_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- Small variations in the data can result in a completely different tree\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Can create biased trees if the classes are highly imbalanced\n",
    "\n",
    "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
